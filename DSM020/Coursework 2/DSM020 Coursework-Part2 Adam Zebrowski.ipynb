{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d69f19a0",
   "metadata": {},
   "source": [
    "# Data programming in Python DSM020 - Coursework 2\n",
    "\n",
    "## Adam Zebrowski - 190113494"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a476b95",
   "metadata": {},
   "source": [
    "# Customer Segmentation:  How can high value customers be identified?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab8d5ecb",
   "metadata": {},
   "source": [
    "## Introduction/Context\n",
    "\n",
    "This project will analyse the Online Retail Data Set from the UCI Machine Learning Repository that can be found here: https://archive.ics.uci.edu/ml/datasets/Online+Retail. \n",
    "The data contains records of transactions between 01/12/2010 and 09/12/2011 for a UK based online site in an Excel format.\n",
    "\n",
    "The aim of the project is to analyse the purchases and transactions made by the  approximately 4000 customers in the datasaet to develop a model for identifying customer segment groups and categories. \n",
    "Doing this will help the business to better understand the needs of each customer category and also identify its most valuable customers in order to grow this segment.\n",
    "\n",
    "Initial analysis of the dataset will help to provide an overview that will be helpful in devising appropriate algorithms for building an effective classification model for the task.\n",
    "\n",
    "This exercise will focus on acquiring and preparing the dataset for further analysis.\n",
    "The dataset is available for free download on the UCI website and has been anonymised for widespread machine learning research and modelling.\n",
    "\n",
    "First step is to explore the data to understand its structure.\n",
    "This includes processing and preparation of the data for summary statistics and visualisations.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e608e3c5",
   "metadata": {},
   "source": [
    "### Import Libraries that will be used for data exploration and processing\n",
    "\n",
    "pandas for manipulating and processing of labeled and columnar data\n",
    "numpy for efficient and fast scientific operations on large amount of data \n",
    "matplotlib for graphing of data\n",
    "seaborn for data visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e19921",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import copy\n",
    "import itertools\n",
    "import datetime as dt\n",
    "\n",
    "\n",
    "#For Use in Part 2 of the Project Below\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans \n",
    "#import datetime\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import plotly.express as px\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdfa3f4a",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "Data is in an excel format which provides a tabular strtucture that we can use for dataframe manipulation, numerical and statistical analysis.\n",
    "\n",
    "Information on whether there are null values in the dataset was not available on the UCI website (N/A). Therefore we have to check for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94dbe839",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the data file\n",
    "\n",
    "dfRetailData = pd.read_excel(\"Online Retail.xlsx\", sheet_name = \"Online Retail\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d52caf",
   "metadata": {},
   "source": [
    "### Clean up dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d48f6b69",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#View the first lines of dataset to check the contents\n",
    "\n",
    "dfRetailData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b4d3726",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Obtain information about the different columns in the dataset including \n",
    "\n",
    "dfRetailData.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7052482e",
   "metadata": {},
   "source": [
    "There are 8 columns in the dataset and 541909 instances as indicated in UCI dataset description.\n",
    "\n",
    "There appears to be some null values for Description and Customer ID.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddedf095",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check column distribution of null values and their proportions\n",
    " \n",
    "Column_Info= pd.DataFrame(dfRetailData.dtypes).T.rename(index={0:'column type'})\n",
    "Column_Info= Column_Info.append(pd.DataFrame(dfRetailData.isnull().sum()).T.rename(index={0:'null values (nb)'}))\n",
    "Column_Info= Column_Info.append(pd.DataFrame(dfRetailData.isnull().sum()/dfRetailData.shape[0]*100).T.\n",
    "                         rename(index={0:'null values (%)'}))\n",
    "display(Column_Info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9966b1f",
   "metadata": {},
   "source": [
    "Almost 25% of the dataset does not have a customer ID.  Looking at the structure of the datasey it will be difficult to replace these null values based on the available information given.  These will be removed\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23204f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Remove Null Values\n",
    "dfRetailData.dropna(axis = 0, subset = ['CustomerID'], inplace = True)\n",
    "\n",
    "#Check null values in dataset and technical info on the columns\n",
    "dfRetailData.isnull().sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c00efeef",
   "metadata": {},
   "source": [
    "It appears removing the null 'CustomerID' values also removed the null 'Description' values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de685f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check dataset dimensions\n",
    "print('Dataframe dimensions:', dfRetailData.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187474e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find duplicates in dataset and delete them\n",
    "\n",
    "print('Number of Duplicate Entries: {}'.format(dfRetailData.duplicated().sum()))\n",
    "dfRetailData.drop_duplicates(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef8a9ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert nominal types described in UCI source websites into categories\n",
    "\n",
    "dfRetailData['CustomerID'] = dfRetailData['CustomerID'].astype('int').astype('category')\n",
    "\n",
    "# Turning object columns into categories also reduces used memory\n",
    "categories = ['InvoiceNo', 'StockCode', 'Description', 'Country']\n",
    "for c in categories:\n",
    "    dfRetailData[c] = dfRetailData[c].astype('category')\n",
    "print(dfRetailData.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ffc0119",
   "metadata": {},
   "source": [
    "Note that memory usage has gone from 33.1+ MB to 16.8MB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b55bb4",
   "metadata": {},
   "source": [
    "### Explore Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95781096",
   "metadata": {},
   "source": [
    "#### 1.0  Dataset Overview\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2fe3785",
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of unique values\n",
    "n_unique = dfRetailData.nunique()\n",
    "print(\"Number of unique values:\\n{}\".format(n_unique))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f40c588e",
   "metadata": {},
   "source": [
    "There are a total of 4372 customers from 37 different countries  and 3684 different products available"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ab0361",
   "metadata": {},
   "source": [
    "##### 1.2 Overview of purchases from different countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be7dd2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot the distribution of customers by country \n",
    "plt.figure(figsize=(15,8))\n",
    "#Use horizontal bar chart of type 'barh'\n",
    "dfRetailData.groupby('Country')['CustomerID'].agg('count').sort_values().plot(kind='barh')\n",
    "plt.title(\"Countries with most No of Customers\",fontsize=18)\n",
    "plt.xlabel(\"Count\",fontsize=14)\n",
    "plt.ylabel(\"Countries\",fontsize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c41654",
   "metadata": {},
   "source": [
    "It is observed that the large proportion of customers come from the United Kingdom.  Germany follows distantly then France and EIRE.  Spain, Netherlands and Belguim then come after these leading countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f527843",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Copy data for more analysis\n",
    "data = dfRetailData.copy()\n",
    "data['Total'] = data['Quantity'] * data['UnitPrice']\n",
    "data['Invoice_Year'] = data['InvoiceDate'].dt.year\n",
    "data['Invoice_Month'] = data['InvoiceDate'].dt.month\n",
    "data['Invoice_Day'] = data['InvoiceDate'].dt.day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7ada52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check purchase value (Total Sales) by country\n",
    "plt.figure(figsize=(15,8))\n",
    "data.groupby('Country')['Total'].sum().sort_values().plot(kind='barh', color=['black', 'red', 'green', 'blue', 'cyan'])\n",
    "plt.title(\"Total Purchase Value by Country\",fontsize=18)\n",
    "plt.xlabel(\"Purchase Value\",fontsize=14)\n",
    "plt.ylabel(\"Countries\",fontsize=14)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4861ae64",
   "metadata": {},
   "source": [
    "It is observed that the UK remains the most valuable in terms of value of purchases.  \n",
    "However, Netherlands comes 2nd in purchase value despite having the 6th most number of customers and Australia comes 6th in terms of purchase value while 10th in terms of number of customers.  I\n",
    "\n",
    "It is noted that Germany and France show lesser value of purchases compared to number of customers from the country.\n",
    "\n",
    "**This is something to note for future analysis during the modeling and classification phase for identifying location of high value customers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af9a636",
   "metadata": {},
   "source": [
    "##### 1.3  Overview of Total Sales with Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32dda8f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sales performance each year\n",
    "per_year_total = data.groupby('Invoice_Year')['Total'].sum()\n",
    "per_year_total.plot(kind='bar')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "432a6d34",
   "metadata": {},
   "source": [
    "Of the two years in the dataset, most of the sales occured in 2011.  Let's dig deeper into this by checking how sales were distributed monthly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0724a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(x='Invoice_Month', y='Total', data=data, hue='Invoice_Year')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a485d0",
   "metadata": {},
   "source": [
    "The barplot shows that sales was only recorded for the 12th month in 2010.  The data for the 12th month is significantly spread out with outliers as shown with the error bar.  \n",
    "\n",
    "*This should be investigated for futher cleaning of data as well as month 1 and 6 for 2011."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c723e2ba",
   "metadata": {},
   "source": [
    "#### 2.0 Customer and Purchasing Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a07620eb",
   "metadata": {},
   "source": [
    "##### 2.1 Products with the Most Purchase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c6c141",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot to see the products with the most purchase in the datset\n",
    "plt.figure(figsize=(10, 8))\n",
    "top_stock_total = data.groupby('StockCode')['Total'].sum().sort_values(ascending=False)[:20]\n",
    "top_stock_total.plot(kind='bar')\n",
    "plt.ylabel('Total Sales')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe387a9",
   "metadata": {},
   "source": [
    "Product with stock code 2243 produced the most sales in the time period within the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61da802f",
   "metadata": {},
   "source": [
    "##### 2.2 Group Customers by Quantity Bought\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6848181",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find top customers\n",
    "\n",
    "top_customers = data.groupby('CustomerID')['Quantity'].sum()\n",
    "top_customers = top_customers.sort_values(ascending=False).head(10)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(8,4))\n",
    "top_customers.plot(kind='bar', color='red')\n",
    "plt.title('Top 10 cutomers by quantity bought')\n",
    "plt.xlabel('Customer ID')\n",
    "plt.ylabel('Quantity')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e4885ea",
   "metadata": {},
   "source": [
    "##### 2.2 Group Customers by Value of Products Bought"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c3bf7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_customers = data.groupby('CustomerID')['Total'].sum()\n",
    "top_customers = top_customers.sort_values(ascending=False).head(10)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(8,4))\n",
    "top_customers.plot(kind='bar', color='blue')\n",
    "plt.title('Top 10 cutomers by Sales Value ')\n",
    "plt.xlabel('Customer ID')\n",
    "plt.ylabel('Sales Value')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acaa75fa",
   "metadata": {},
   "source": [
    "It is observed that from the two plots above that some customers buy less quantity but spend more"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b15a533c",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "A number of data cleaning operatins were performed on the dataset to remove null values and ensure that the definitions of the columns as categories were preserved.  These actions also improved the efficiency of the data analysis.  \n",
    "It was observed from data exploration that further analysis for modeling and classificatiion in order to identify high value customers should not only consider quantity of items bought but also their values. \n",
    "In addition to specifically identifying high value customers based on Customer ID, it would also be useful to relate high value customers in terms of quantity bought and sales value per customers to the specific countries.\n",
    "This will help in developing and growing the customer base for international retail sales outside the UK."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c591328f",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "Anaconda, 2021b. NumPy Documentation. [Online] \n",
    "Available at: https://numpy.org/doc/stable/user/whatisnumpy.html\n",
    "[Accessed 13th June 2021].\n",
    "\n",
    "Anaconda, 2021c. mapplotlib Documentation. [Online] \n",
    "Available at: https://matplotlib.org/stable/tutorials/introductory/usage.html#sphx-glr-tutorials-introductory-usage-py\n",
    "[Accessed 13th June 2021].\n",
    "\n",
    "Anaconda, 2021d. Categorical data. [Online] \n",
    "Available at: https://pandas.pydata.org/pandas-docs/stable/user_guide/categorical.html\n",
    "[Accessed 15th June 2021].\n",
    "\n",
    "Anaconda, 2021. pandas Documentation. [Online] \n",
    "Available at: https://pandas.pydata.org/docs/getting_started/index.html\n",
    "[Accessed 13th June 2021].\n",
    "\n",
    "Chen, D., Sain, S. L. & Guo, K., 2012 . Data mining for the online retail industry: A case study of RFM model-based customer segmentation using data mining. Journal of Database Marketing and Customer Strategy Managemen, 19(3), pp. 197-208.\n",
    "Harris, A., 2021. The difficulties with pandas categories. [Online] \n",
    "Available at: https://towardsdatascience.com/staying-sane-while-adopting-pandas-categorical-datatypes-78dbd19dcd8a\n",
    "[Accessed 15th June 2021].\n",
    "\n",
    "Pathak, M., 2020. Handling Categorical Data in Python. [Online] \n",
    "Available at: https://www.datacamp.com/community/tutorials/categorical-data#categorical\n",
    "[Accessed 14th June 2021].\n",
    "\n",
    "UCI Machine Learning Repository, 2021. Online Retail Data Set. [Online] \n",
    "Available at: https://archive.ics.uci.edu/ml/datasets/Online+Retail#\n",
    "[Accessed 13th June 2021].\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ac27b1",
   "metadata": {},
   "source": [
    "# Part 2:  Analysis\n",
    "\n",
    "This section builds on the exploratory analysis above.  Here, an in-depth analysis of the 'Online Retail' dataset is undertaken through further data processing, statistical and visualisaiton techniques to evaluate and summarise the data towards achieving the aims and objectives of the project. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec0dd169",
   "metadata": {},
   "source": [
    "### Project Aims and Objectives\n",
    "\n",
    "Aims: \n",
    "\n",
    "Gain greater understanding of the nature of the customers and their behaviuour through analysis of transactions recorded in the dataset.\n",
    "\n",
    "Identify if customers can be segmented meaningfully into groups within the dataset.  \n",
    "This will help to gain greater understanding of their needs and allow tailoring of products and services to better satisfy customer needs. \n",
    "\n",
    "Recognise the most valuable customers and their characteristics in order to design strategies to grow this segment for increased sales and profits\n",
    "\n",
    "Objectives:\n",
    "\n",
    "What are the purchasing behaviours of the customers:\n",
    "\n",
    "Identify customer average monetary expenditure on purchases\n",
    "Understand the frequency of customer transactions \n",
    "Analyse and identify segment groups within the dataset based on purchasing behaviour.  This will help evaluate the value of different customer segments to the retailer.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "035d3183",
   "metadata": {},
   "source": [
    "### Add new Libraries \n",
    "\n",
    "New libraries added to the Import libraries section at the top of part 1:\n",
    "\n",
    "StandardScaler for standardising features for machine learning\n",
    "KMeans for clustering \n",
    "timedelta for calculating time duration\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "362a639b",
   "metadata": {},
   "source": [
    "## Pre-processing for further analysis\n",
    "Data pre-processing is required to remove any returned purchases (negative quantities) or items given away for free (zero price) in order to analyse customer value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d57e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove rows with no quantities (Cancelled orders with negative or zero quantities)\n",
    "NoQuantities = data[data['Quantity'] <= 0].index\n",
    "data.drop(NoQuantities, inplace=True )\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ccd828d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove rows with no unit price. These could have been free items\n",
    "NoQuantities = data[data['UnitPrice'] <= 0].index\n",
    "data.drop(NoQuantities, inplace=True )\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "befa3d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove rows with no income (Zero or negative monetary purchases). These could have been broken, thrown away, etc.\n",
    "\n",
    "\n",
    "Noincome = data[data['Total'] <= 0].index\n",
    "dfRetailData.drop(Noincome, inplace=True )\n",
    "dfRetailData.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f252ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use scatterplot to check for outliers for both unit price and quantities purchased\n",
    "plt.scatter(data['Quantity'], data['UnitPrice'])\n",
    "plt.xlabel(\"Quantity\",fontsize=14)\n",
    "plt.ylabel(\"Unit Price\",fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2739a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "#There appear to be outlier/s above 5000 for unit price \n",
    "#Let's take a closer look\n",
    "\n",
    "data[data['UnitPrice'] > 5000]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96ac0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking outliers above 20,000 for Quantity\n",
    "data[data['Quantity'] > 20000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c2a8184",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These outliers total 3 rows in the whole dataset and can be removed to reduce skewness during analysis\n",
    "\n",
    "# Treat the extreme values as outliers and remove them\n",
    "Extremeoutliers = data[(data['UnitPrice'] > 5000) | (data['Quantity'] > 20000)].index\n",
    "data.drop(Extremeoutliers, inplace = True)\n",
    "\n",
    "#Review the dataset \n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e561cd",
   "metadata": {},
   "source": [
    "## Evaluation of Customer Purchasing Behaviour\n",
    "Evalution of customer behaviour will be based on the RFM model which enables identification of valuable customer based on their purchasing patterns in terms of :\n",
    "\n",
    "How recently they have purchased (Recency R)]\n",
    "Frequency of purchases (F)\n",
    "Monetary value of purchases (M)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5efcbb0a",
   "metadata": {},
   "source": [
    "#### Create RFM Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b774fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#format date\n",
    "data['InvoiceDate'] = pd.to_datetime(data['InvoiceDate'], dayfirst=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89658cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select Rows from 2011 since this contains most sales and covers 12 months of sales\n",
    "df_2011 = data[data['Invoice_Year'] == 2011]\n",
    "df_2011.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c6852fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate recency using the day after last invoice date in dataset as the reference marker snapshot \n",
    "\n",
    "#Create snapshot date variable\n",
    "\n",
    "SnapshotDate = df_2011['InvoiceDate'].max() + timedelta(days=1)\n",
    "print(SnapshotDate)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f8a006",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate RFM values grouped per customer\n",
    "\n",
    "CustomerData = df_2011.groupby(['CustomerID']).agg({\n",
    "    'InvoiceDate': lambda x: (SnapshotDate - max(x)).days,\n",
    "    'InvoiceNo': 'count',\n",
    "    'Total': 'sum'})\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d9ce39",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rename the columns to reflect RFM labels\n",
    "\n",
    "\n",
    "CustomerData.rename(columns = {'InvoiceDate': 'Recency',\n",
    "                            'InvoiceNo': 'Frequency',\n",
    "                            'Total': 'MonetaryValue'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "689c2b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove any NaN values\n",
    "CustomerData.dropna(axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2e271a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#View details of the customer RFM table\n",
    "\n",
    "print(CustomerData.head())\n",
    "print('{:,} rows; {:,} columns'.format(CustomerData.shape[0],CustomerData.shape[1])\n",
    "    ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "776e7623",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To understand how customer value using RFM, clustering unsupervised machine learning algorithm k-means will be applied\n",
    "# Check to make sure data is not skewed before applying features to machine learing\n",
    "\n",
    "#Plot RFM histogram distributions\n",
    "\n",
    "plt.figure(figsize=(16,12))\n",
    " \n",
    "    #Plot Histogram distribution for R\n",
    "plt.subplot(3,1,1);sns.histplot(data=CustomerData,x=CustomerData['Recency'])\n",
    "\n",
    "#Plot Histogram distribution for F\n",
    "plt.subplot(3,1,2);sns.histplot(data=CustomerData,x=CustomerData['Frequency'])\n",
    "\n",
    "#Plot Histogram distribution for M\n",
    "plt.subplot(3,1,3);sns.histplot(data=CustomerData, x=CustomerData['MonetaryValue'])\n",
    "\n",
    "\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ae1c1d",
   "metadata": {},
   "source": [
    "Examining the histograms of the RFM variables, it is shown that the scales for the three differenct features vary considerably.  \n",
    "This would have to be standardised before applying machine learning algorithm like k-means clustering.  \n",
    "This is because k-means is very sensitive to scales that are not comparable.  \n",
    "StandardScaler from Scikit-Learn will be used to further reduce the range of the dataset to a predetermined scale.\n",
    "This is applicable here since outliers have been minimised in earlier pre-processing\n",
    "\n",
    "Also distributions of features that are approximately normal improve machine learning performance.\n",
    "Quantile Transformaion from Scikit-Learn will be used to standardise the data bringing the standard deviation towards 1 (unit variance).  This tends the shape of the data towards normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b079f45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Copy Customer RFM data to use for transormation\n",
    "StCustomerData = CustomerData.copy()\n",
    "\n",
    "#View Statistical summary for RFM\n",
    "StCustomerData.describe()    #This inludes the mean and stadard deviation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f962f7f1",
   "metadata": {},
   "source": [
    "#### Reduce skewness of RFM variables for k-means machine Learnign analysis\n",
    "\n",
    "Quantile Transformation is applied.  This method allows the features to follow a uniform or normal distribution after transformation (Gaussian with 0 mean and unit variance). This reduces misleading results due to outliers and improves performance for statistical analysis and machine learning algorithms .\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab42d65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply Quantile Transformation to normalise Recency data\n",
    "\n",
    "VTransform = QuantileTransformer(n_quantiles=100, random_state=1111, output_distribution='normal')\n",
    "VTransform.fit(CustomerData[['Recency']])\n",
    "StCustomerData['Recency_RankGauss'] = VTransform.transform(CustomerData[['Recency']])\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1,2, figsize=[12,5])\n",
    "ax1.hist(CustomerData['Recency'],bins=100)\n",
    "ax1.title.set_text('Original Data')\n",
    "ax2.hist(StCustomerData['Recency_RankGauss'],bins=100)\n",
    "ax2.title.set_text('RankGauss-ed Data')\n",
    "plt.show()\n",
    "\n",
    "#Display skewness after transformation\n",
    "print(\"Skew Original: \", CustomerData['Recency'].skew().round(2))\n",
    "print(\"Skew RankGauss-ed Data: \", StCustomerData['Recency_RankGauss'].skew().round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3dfb35b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply Quantile Transformation to normalise Frequency data\n",
    "\n",
    "VTransform = QuantileTransformer(n_quantiles=100, random_state=1111, output_distribution='normal')\n",
    "VTransform.fit(CustomerData[['Frequency']])\n",
    "StCustomerData['Frequency_RankGauss'] = VTransform.transform(CustomerData[['Frequency']])\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1,2, figsize=[12,5])\n",
    "ax1.hist(CustomerData['Frequency'],bins=100)\n",
    "ax1.title.set_text('Original Data')\n",
    "ax2.hist(StCustomerData['Frequency_RankGauss'],bins=100)\n",
    "ax2.title.set_text('RankGauss-ed Data')\n",
    "plt.show()\n",
    "print(CustomerData['Frequency'].skew().round(2))\n",
    "print(StCustomerData['Frequency_RankGauss'].skew().round(2))\n",
    "\n",
    "#Display skewness after transformation\n",
    "print(\"Skew Original: \", CustomerData['Frequency'].skew().round(2))\n",
    "print(\"Skew RankGauss-ed Data: \", StCustomerData['Frequency_RankGauss'].skew().round(2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c516309",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply Quantile Transformation to normalise MonetaryValue\n",
    "\n",
    "VTransform = QuantileTransformer(n_quantiles=100, random_state=1111, output_distribution='normal')\n",
    "VTransform.fit(CustomerData[['MonetaryValue']])\n",
    "StCustomerData['MonetaryValue_RankGauss'] = VTransform.transform(CustomerData[['MonetaryValue']])\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1,2, figsize=[12,5])\n",
    "ax1.hist(CustomerData['MonetaryValue'],bins=100)\n",
    "ax1.title.set_text('Original Data')\n",
    "ax2.hist(StCustomerData['MonetaryValue_RankGauss'],bins=100)\n",
    "ax2.title.set_text('RankGauss-ed Data')\n",
    "plt.show()\n",
    "\n",
    "#Display skewness after transformation\n",
    "print(\"Skew Original: \", CustomerData['MonetaryValue'].skew().round(2))\n",
    "print(\"Skew RankGauss-ed Data: \", StCustomerData['MonetaryValue_RankGauss'].skew().round(2))\n",
    "                                                               \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8c7086",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove Columns with Original Data\n",
    "\n",
    "StCustomerData.drop(['Recency','Frequency','MonetaryValue'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d2b8a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check dataset\n",
    "\n",
    "StCustomerData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c25a3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rename Columns to RFM\n",
    "\n",
    "StCustomerData.rename(columns={\"Recency_RankGauss\":\"Recency\",\"Frequency_RankGauss\":\"Frequency\",\"MonetaryValue_RankGauss\":\"MonetaryValue\"},inplace=True)\n",
    "#Check Results\n",
    "StCustomerData.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf3c086b",
   "metadata": {},
   "source": [
    "#### Reduce Scale range of dataset features for k-means machine learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "560ca83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scale to normalise data\n",
    "\n",
    "data_scaler = StandardScaler()\n",
    "\n",
    "# Fit and Transform The Data\n",
    "NormCustomerData = data_scaler.fit_transform(StCustomerData) #This returns a numbpy array\n",
    "\n",
    "# Check that the features a normalised with mean 0 and variance 1\n",
    "print(NormCustomerData.mean(axis = 0).round(2)) \n",
    "print(NormCustomerData.std(axis = 0).round(2)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9cc6e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Veiw Data\n",
    "\n",
    "#Convert numpy array outputed from StandardScaler() fit-tranfsorm \n",
    "\n",
    "NormCustomerData = pd.DataFrame(NormCustomerData)\n",
    "\n",
    "#Check Statistical Summary.  See if mean converged to 0 and variance 1 compared to earlier un-normalised features\n",
    "NormCustomerData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22390f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rename Columns to RFM\n",
    "\n",
    "NormCustomerData.rename(columns={0:\"Recency\",1:\"Frequency\",2:\"MonetaryValue\"},inplace=True)\n",
    "#Check Results\n",
    "NormCustomerData.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "817c93b3",
   "metadata": {},
   "source": [
    "### Clustering/Segmentation Model and Algorithm:  K-Means\n",
    "\n",
    "Comparing clustering models for identying customer segment groups, K-Means appears to be the most suitable  as it is scalable for the  large dataset being analysed and produces not too many clusters: https://scikit-learn.org/stable/modules/clustering.html.  \n",
    "This very applicable for customer segmentation for which a large number of clusters/segments can be confusing and impracticable from a business point of view.  \n",
    "The model applies a powerful unsupervised machine learning algorithm for intrinsically finding groups in unlabelled data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee22308",
   "metadata": {},
   "source": [
    "#### K-Means Model Implementation\n",
    "\n",
    "The algorithm requires using an optimal number of pre-defined K value for best performance in find clusters.\n",
    "This is because the performance of the algorithm will depend on the chosen K value.\n",
    "It is best to choose this optimal K value by testing the K-Means algorithm for different values of K and comparing the results.\n",
    "\n",
    "A very popular and common technique for determining optimal K value is the elbow method.  This will be used in the analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e512a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determin optimal K using Elbow Method\n",
    "# The Elbow Method works by plotting cost functions of different K values. \n",
    "#The elbow is the inflection point at which dispersion between the clusters decline of dispersion \n",
    "#begins to deminish the most after and inflection point\n",
    "\n",
    "\n",
    "cs = []\n",
    "for i in range(1, 11):\n",
    "    kmeans = KMeans(n_clusters = i, init = 'k-means++', max_iter = 300, n_init = 10, random_state = 0)\n",
    "    kmeans.fit(NormCustomerData)\n",
    "    cs.append(kmeans.inertia_) #inertia measures how well Kmeans is able to cluster dataset\n",
    "plt.plot(range(1, 11), cs)\n",
    "plt.title('The Elbow Method')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('Dispersion')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "649d28cc",
   "metadata": {},
   "source": [
    "##### Testing K-Means Performance with Potential Optimal Clusters\n",
    "\n",
    "There is a kink at 2 which can be a good number of K to start. \n",
    "It is also observed that changes in dispersion begins to level off after 4.\n",
    "\n",
    "We first try clustering with K of 4 to observe the clustering of the model with this.  \n",
    "Clustering of the model with K of 3 and 5, one below and one above will also provide and overview of Model clusters for the different K"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee05369d",
   "metadata": {},
   "source": [
    "###### K-Means with 4 Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "640dbe1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=4, random_state=0)\n",
    "\n",
    "kmeans.fit(NormCustomerData)\n",
    "y_kmeans = kmeans.predict(NormCustomerData)\n",
    "\n",
    "labels = kmeans.labels_\n",
    "CustomerData[\"Cluster\"] = labels\n",
    "print('Number of data points in each cluster= \\n',CustomerData['Cluster'].value_counts())\n",
    "\n",
    "\n",
    "\n",
    "# View Section of Clustering Assignments to Customers\n",
    "labels = kmeans.labels_\n",
    "CustomerData[\"Cluster\"] = labels\n",
    "CustomerData.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9299c722",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Plot for Visualisation of Clusters with K =4\n",
    "\n",
    "px.scatter_3d(NormCustomerData, x='Recency', y='Frequency', z='MonetaryValue', color=labels,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0bb3b32",
   "metadata": {},
   "source": [
    "##### K-Means with 3 Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad71b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=3, random_state=0)\n",
    "\n",
    "kmeans.fit(NormCustomerData)\n",
    "y_kmeans = kmeans.predict(NormCustomerData)\n",
    "\n",
    "labels = kmeans.labels_\n",
    "CustomerData[\"Cluster\"] = labels\n",
    "print('Number of data points in each cluster= \\n',CustomerData['Cluster'].value_counts())\n",
    "\n",
    "\n",
    "\n",
    "# View Section of Clustering Assignments to Customers\n",
    "labels = kmeans.labels_\n",
    "CustomerData[\"Cluster\"] = labels\n",
    "CustomerData.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dfbeeeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot for Visualisation of Clusters with K =3\n",
    "import plotly.express as px\n",
    "px.scatter_3d(NormCustomerData, x='Recency', y='Frequency', z='MonetaryValue', color=labels,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "278799d8",
   "metadata": {},
   "source": [
    "##### K-Means with 5 Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597cb859",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=5, random_state=0)\n",
    "\n",
    "kmeans.fit(NormCustomerData)\n",
    "y_kmeans = kmeans.predict(NormCustomerData)\n",
    "\n",
    "labels = kmeans.labels_\n",
    "CustomerData[\"Cluster\"] = labels\n",
    "\n",
    "print('Number of data points in each cluster= \\n',CustomerData['Cluster'].value_counts())\n",
    "\n",
    "\n",
    "\n",
    "# View Section of Clustering Assignments to Customers\n",
    "labels = kmeans.labels_\n",
    "CustomerData[\"Cluster\"] = labels\n",
    "CustomerData.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ca8389",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot for Visualisation of Clusters with K =5\n",
    "import plotly.express as px\n",
    "px.scatter_3d(NormCustomerData, x='Recency', y='Frequency', z='MonetaryValue', color=labels,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4737761",
   "metadata": {},
   "source": [
    "It would appear that segmentation with 5 clusters provides clear, distinct view of different groups.\n",
    "This will be double checked by testing KMeans with 6 clusters\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da54df59",
   "metadata": {},
   "source": [
    "##### K-Means with 6 Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f41859",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=6, random_state=0)\n",
    "\n",
    "kmeans.fit(NormCustomerData)\n",
    "y_kmeans = kmeans.predict(NormCustomerData)\n",
    "\n",
    "labels = kmeans.labels_\n",
    "CustomerData[\"Cluster\"] = labels\n",
    "print('Number of data points in each cluster= \\n',CustomerData['Cluster'].value_counts())\n",
    "\n",
    "\n",
    "\n",
    "# View Section of Clustering Assignments to Customers\n",
    "labels = kmeans.labels_\n",
    "CustomerData[\"Cluster\"] = labels\n",
    "CustomerData.head()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229121b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot for Visualisation of Clusters with K =6\n",
    "import plotly.express as px\n",
    "px.scatter_3d(NormCustomerData, x='Recency', y='Frequency', z='MonetaryValue', color=labels,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a19406",
   "metadata": {},
   "source": [
    "It is observed from above, that K-Means with 6 clusters was only marginally different from the one with 5 clusters so K = 5 appears optimal for this dataset and will be used for analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde58d7a",
   "metadata": {},
   "source": [
    "### Cluster and Customer Segment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07998cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Group Customers according to clusters using optimal K=5 data\n",
    "#Rerun code for K = 5 to avoid errors\n",
    "kmeans = KMeans(n_clusters=5, random_state=0)\n",
    "\n",
    "kmeans.fit(NormCustomerData)\n",
    "y_kmeans = kmeans.predict(NormCustomerData)\n",
    "\n",
    "labels = kmeans.labels_\n",
    "CustomerData[\"Cluster\"] = labels\n",
    "\n",
    "print('Number of data points in each cluster= \\n',CustomerData['Cluster'].value_counts())\n",
    "\n",
    "\n",
    "\n",
    "# View Section of Clustering Assignments to Customers\n",
    "labels = kmeans.labels_\n",
    "CustomerData[\"Cluster\"] = labels\n",
    "CustomerData.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca611c5",
   "metadata": {},
   "source": [
    "###### Section Analysis Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e9b392d",
   "metadata": {},
   "source": [
    "As shown above the largest segment of customers belong to cluster 2, made up of 1709 customers and about 41% of cuaatomers.  This segment is followed by clusters 0 and 1 respectively.  Clusters 4 and 3 are relatively small compared to the other 3 making up no more than 4 percent of customerscombined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba67df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Group Customers by Clusters with Mean statistics Summary\n",
    "CustomerData.groupby('Cluster').agg({\n",
    "    'Recency':'mean',\n",
    "    'Frequency':'mean',\n",
    "    'MonetaryValue':['mean', 'count']}).round(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e1ad4c",
   "metadata": {},
   "source": [
    "###### Segement Analysis by Mean Sumamry\n",
    "\n",
    "The Table directly above certainly indicates the following:\n",
    "\n",
    "1. Largest segment of customers, cluster 2 making up 41% of customers does not on the average represent significant monetary value for the company.  In fact they are second to last in terms of average monetary value, only behind segment 3 which is only 2% of customers. Relative to other segments the frequency of purchase of this large segment is medium at best and average recency of purchase is at least 2 months from the last invoice date.\n",
    "\n",
    "2. Segment 4 is quite interesting as it appears to be a very new segment showing vthe highest frequency of purchase in the dataset and also the highest monetary value of purchases despite making up less than 2% of the dataset.  This segment needs to be analysed in greater detail to understand the group better for purposes of strategic targeting \n",
    "\n",
    "3. Segment 1 which makes up the third largest group at about 25% of customers appears promising as a group.  On the average, the segment pruchases frequently and is second only to section 4 in recency, frequency and monetary value.\n",
    "\n",
    "4.  Segment  0 apperas to be a particularly difficult group group what has not purchased in the last five months on the average, represent very low frequency of purchase and very low monetary value.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728c24db",
   "metadata": {},
   "outputs": [],
   "source": [
    "ClusterAvg = CustomerData.groupby('Cluster').mean()\n",
    "PopulationAvg = CustomerData.mean()\n",
    "RelativeImp = ClusterAvg / PopulationAvg - 1\n",
    "RelativeImp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "726d52f4",
   "metadata": {},
   "source": [
    "###### Digging Deeper into the Mean RFM\n",
    "\n",
    "Deeper analysis into how the RFM means relate to the average population of customers, the followin were observed in terms of relative importance of each customer group, the following can be concluded:\n",
    "\n",
    "1.  Segments 1 and 4 represent the most valuable customers within the dataset. They both show relative positive frequency and monetary value increase and reduced recency interval in comparison to others.  \n",
    "\n",
    "2.  Segment 2, the largest appear to be the most loyal.  Frequency and monetary average are less than the average population of customers but not strongly.  In addition their purchasee recency is only a little less than the general population of customers.\n",
    "\n",
    "3.The analysis above also confirms the significant problem with segment 0 with far less reduced frequency and monetary value compared to other customer segments and very high recency interval duration of last purchase.\n",
    "\n",
    "4.  Segment 3 only makes up 2% of customers, have purchased the least recent of all customers on the average and do not respresent much customer value in terms of frequency and monetary value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d3b107",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b08d5bb5",
   "metadata": {},
   "source": [
    "Key aims of the project were to analyse the dataset to provide clearer understanding of the behaviour of different customers, be able to identify important segment groups to assist the business with strategic marketing decisions and growing the company.\n",
    "\n",
    "Towards these aims, objectives included been able to identify and segment customers meaningfully based on patterns of behaviour.  The RFM analysis in addition to K-Means clustering and analysis was used to acheive this.  Five key\n",
    "patterns were identified and clearly defined with important differentiating behaviours which will be very useful in helping the business in strategic marketing and planning.\n",
    "The cluster analysis also provided important insights for increased understanding and recognition of the most valueable customer groups and their monetary value, frequency and recency of purchase in relation to other customers on the average which was a key objective for the project.\n",
    "\n",
    "The results of this analysis has been able to provide the necessary tools for the business to achieve the aims and objectives stated for this project.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9babe1e1",
   "metadata": {},
   "source": [
    "## Create Sample Data for Submission\n",
    "\n",
    "\n",
    "\n",
    "Create a sample of 120,000 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d44d1446",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dfSample = dfRetailData[:150000]\n",
    "#Check size\n",
    "print(dfSample.shape)\n",
    "dfSample.info()\n",
    "#dfRetailData.memory_usage(index=True).sum()\n",
    "#dfSample.head()\n",
    "\n",
    "#Export to Excel\n",
    "\n",
    "dfSample.to_excel(\"samplePart2.xlsx\", sheet_name='Retail Data')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aaecec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clear history to release memory\n",
    "%reset Out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31274bc5",
   "metadata": {},
   "source": [
    "## References: Part 2\n",
    "\n",
    "Brownlee, J., 2020. How to Use StandardScaler and MinMaxScaler Transforms in Python. [Online] \n",
    "Available at: https://machinelearningmastery.com/standardscaler-and-minmaxscaler-transforms-in-python/\n",
    "[Accessed 16th August 2021].\n",
    "Jackson, S., 2019. Feature Transformation. [Online] \n",
    "Available at: https://medium.com/@sjacks/feature-transformation-21282d1a3215\n",
    "[Accessed 16th August 2021].\n",
    "Joe, J., 2021. [Online] \n",
    "Available at: https://iterable.com/blog/the-secret-to-customer-lifetime-value-rfm/\n",
    "[Accessed 14th August 2021].\n",
    "Khanna, C., 2020. What and why behind fit_transform() and transform() in scikit-learn!. [Online] \n",
    "Available at: https://towardsdatascience.com/what-and-why-behind-fit-transform-vs-transform-in-scikit-learn-78f915cf96fe\n",
    "[Accessed 16th August 2021].\n",
    "scikit-learn, 2021. [Online] \n",
    "Available at: https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html\n",
    "[Accessed 14th August 2021].\n",
    "scikit-learn, 2021b. Compare the effect of different scalers on data with outliers. [Online] \n",
    "Available at: https://scikit-learn.org/stable/auto_examples/preprocessing/plot_all_scaling.html\n",
    "[Accessed 15th August 2021].\n",
    "scikit-learn, 2021c. sklearn.preprocessing.QuantileTransformer. [Online] \n",
    "Available at: https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.QuantileTransformer.html\n",
    "[Accessed 15th August 2021].\n",
    "scikit-learn, 2021d. Preprocessing data. [Online] \n",
    "Available at: https://scikit-learn.org/stable/modules/preprocessing.html\n",
    "[Accessed 15th August 2021].\n",
    "towards datascience, 2019. Preprocessing: Differences in Standardization Methods. [Online] \n",
    "Available at: https://towardsdatascience.com/preprocessing-differences-in-standardization-methods-de53d2525a87\n",
    "[Accessed 16th August 2021].\n",
    "Yuan, Y., 2019. Recency, Frequency, Monetary Model with Python — and how Sephora uses it to optimize their Google and Facebook Ads. [Online] \n",
    "Available at: https://towardsdatascience.com/recency-frequency-monetary-model-with-python-and-how-sephora-uses-it-to-optimize-their-google-d6a0707c5f17\n",
    "[Accessed 14th August 2021].\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2475655",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}